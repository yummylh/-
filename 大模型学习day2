æ­å»ºå®Œäº†LeNETå…¨éƒ¨ç½‘ç»œ
ä»Šå¤©åˆšåˆšæ­å»ºå®Œå…¨éƒ¨ç½‘ç»œï¼Œç°åœ¨è¿›è¡Œä¸€ä¸‹å¤ç›˜
é”™è¯¯ä¿¡æ¯æ•´ç†
1.ç¬¬ä¸€ä¸ªé”™è¯¯ä¿¡æ¯å…¶å®è¿˜æ˜¯ç¼©è¿›çš„é—®é¢˜ï¼ˆpythonç¼©è¿›å®³äººä¸æµ…ï¼ï¼‰
ç‰¹åˆ«æ˜¯ä»£ç ä¸­è¡Œ171ä¸­ï¼Œå› ä¸ºtrain_processç¼©è¿›é—®é¢˜å¯¼è‡´ä»£ç æŠ¥é”™
2.å‘ç°è®­ç»ƒè½®æ¬¡ä¸Šå»åæ˜¾å¡ç®—åŠ›ä¸å¤ªå¤Ÿï¼Œå¯èƒ½ä»¥åå¾—ç§Ÿä¸ªè½»é‡çº§æœåŠ¡å™¨ï¼Œå¤§æ¦‚0.7r/hï¼Œä¸ç”¨çš„æ—¶å€™å…³æœºï¼Œåªåœ¨è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ç”¨ï¼Œæ„Ÿè§‰ä¹Ÿä¸ä¼šèŠ±å¤šå°‘é’±ğŸ¤”
ä¸‹ä¸€æ­¥è®¡åˆ’
1.å‡†å¤‡å¼€å§‹transformçš„ç¼–å†™
2.åˆ·leetcode

ä»¥ä¸‹æ˜¯æœ¬æ¬¡ä»£ç 
æ³¨æ„ï¼ŒLeNETæ˜¯å·²ç»ç¼–å†™å¥½çš„LeNETç½‘ç»œï¼Œå…¶æœºæ„å‚æ•°å·²ç»ç¡®å®šï¼ŒåŒ…æ‹¬inputä»¥åŠoutput
import copy
import time

from torchvision.datasets import FashionMNIST
import numpy as np
from torchvision import transforms
import torch.utils.data as Data
import matplotlib.pyplot as plt

# import LeNET
from LeNET import LeNet
import torch
import torch.nn as nn
import pandas as pd

#æ•°æ®åŠ è½½
def train_val_data_process():
    train_data = FashionMNIST(root='./data',
                              train=True,
                              transform=transforms.Compose([transforms.Resize(size=28), transforms.ToTensor()]),
                              download=True)
    train_data,val_data = Data.random_split(train_data,[round(0.8*len(train_data)),round(0.2*len(train_data))])

    train_dataloader = Data.DataLoader(dataset=train_data,
                                       batch_size=32,
                                       shuffle=True,
                                       num_workers=6)

    val_dataloader = Data.DataLoader(dataset=val_data,
                                       batch_size=32,
                                       shuffle=True,
                                       num_workers=6)

    return train_dataloader,val_dataloader

train_dataloader,val_dataloader = train_val_data_process()

#å®šä¹‰æ¨¡å‹åˆå§‹åŒ–å€¼
def train_model_process(LeNET,tran_dataloader,val_dataloader,num_epochs):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    optimizer = torch.optim.Adam(LeNET.parameters(),lr=0.001)#æ¿€æ´»å‡½æ•°

    criterion = nn.CrossEntropyLoss()#æŸå¤±å‡½æ•°


    LeNET = LeNET.to(device)#æ¨¡å‹æ”¾å…¥GPU

    best_model_wts = copy.deepcopy(LeNET.state_dict())


    best_acc = 0.0

    train_loss_all = []

    val_loss_all = []

    train_acc_all = []

    val_acc_all = []

    since = time.time()


    #å¼€å§‹è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒï¼Œå¹¶åˆå§‹åŒ–å‚æ•°ï¼Œæ’°å†™å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€æ¢¯åº¦æ›´æ–°
    for epoch in range(num_epochs):
        print("Epoch{}/{}".format(epoch,num_epochs-1))#ä»0å¼€å§‹è®­ç»ƒæ‰€ä»¥éœ€è¦å‡å»ä¸€
        print("-"*10)

        #åˆå§‹åŒ–å‚æ•°
        train_loss = 0.0
        train_corrects = 0

        val_loss = 0.0
        val_corrects = 0

        #è®­ç»ƒé›†åŠéªŒè¯é›†æ ·æœ¬æ•°é‡
        train_num = 0
        val_num = 0


        #å–æ•°æ®,
        for step,(b_x,b_y) in enumerate(train_dataloader):
            #æ”¾è¿›GPUä¸­
            b_x = b_x.to(device)
            b_y = b_y.to(device)

            LeNET.train()#å¼€å¯æ¨¡å‹è®­ç»ƒæ¨¡å¼

            #å‰å‘ä¼ æ’­ï¼Œå¯¹è¾“å…¥çš„batchè¿›è¡Œæµ‹è¯•ï¼Œè¾“å‡ºå¯¹åº”batchçš„é¢„æµ‹å€¼
            output = LeNET(b_x)

            pre_lab = torch.argmax(output,dim=1)
            #è®¡ç®—æ¯ä¸ªbatchçš„æŸå¤±
            loss = criterion(output,b_y)

            # å°†æ¢¯åº¦ç½®ä¸º 0 æ›´æ–°å‚æ•°ï¼Œå¹¶ä¸”åå‘ä¼ æ’­
            optimizer.zero_grad()
            #åå‘ä¼ æ’­
            loss.backward()
            #å‚æ•°æ›´æ–°
            optimizer.step()

            train_loss += loss.item()*b_x.size(0)#æ¯ä¸ªæ ·æœ¬çš„å¹³å‡losså€¼ï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ªbatch_sizeé‡Œé¢ï¼Œå‡å¦‚æˆ‘æ†128å¼ å›¾ç‰‡è¿›æ¥ï¼Œä¸€ä¸ªå›¾ç‰‡losså€¼0.001ï¼Œé‚£ä¹ˆè¿™ä¸ªbatch_sizeé‡Œé¢lossæ€»å…±å°±æ˜¯0.1ï¼Œä½†æ˜¯ä¸€ä¸ªå®Œæ•´è½®æ¬¡éœ€è¦è®­ç»ƒå¤§çº¦12800å¼ å›¾ï¼Œæ‰€ä»¥æ€»çš„è½®æ¬¡losså°±æ˜¯æ‰€æœ‰batch_size lossç›¸åŠ åœ¨é™¤ä»¥æ€»çš„è½®æ¬¡å›¾12800å³å¯å¾—åˆ°ä¸€ä¸ªè½®æ¬¡çš„losså€¼
            #é¢„æµ‹æ­£ç¡®çš„è¯ï¼Œæ­£ç¡®å€¼+1
            train_corrects += torch.sum(pre_lab == b_y.data)

            train_num += b_x.size(0) #è®­ç»ƒæ ·æœ¬çš„æ€»æ•°é‡ï¼Œä¸€ä¸ªè½®æ¬¡

        for step,(b_x,b_y) in enumerate(val_dataloader):
            #æ”¾è¿›GPUä¸­
            b_x = b_x.to(device)

            b_y = b_y.to(device)

            LeNET.eval()#æ¨¡å‹å¼€å¯éªŒè¯æ¨¡å¼


            output = LeNET(b_x)

            pre_lab = torch.argmax(output, dim=1)

            loss = criterion(output, b_y)

            val_loss += loss.item() * b_x.size(0)
            # é¢„æµ‹æ­£ç¡®çš„è¯ï¼Œæ­£ç¡®å€¼+1
            val_corrects += torch.sum(pre_lab == b_y.data)

            val_num += b_x.size(0)

        # ä¸€ä¸ªepochçš„losså€¼
        train_loss_all.append(train_loss/train_num)
        train_acc_all.append(train_corrects.double().item()/train_num)
        # ä¸€ä¸ªepochçš„losså€¼
        val_loss_all.append(val_loss / val_num)
        val_acc_all.append(val_corrects.double().item() / val_num)


        print('{} Train Loss: {:.4f} Train ACC:{:.4f}'.format(epoch,train_loss_all[-1],train_acc_all[-1]))
        print('{} Val Loss: {:.4f} Val ACC:{:.4f}'.format(epoch, val_loss_all[-1], val_acc_all[-1]))

        if val_acc_all[-1] > best_acc:

            best_acc = val_acc_all[-1]

            best_model_wts = copy.deepcopy(LeNET.state_dict())


        #è®¡ç®—è®­ç»ƒè€—æ—¶
        time_use = time.time()-since
        print("è®­ç»ƒå’ŒéªŒè¯è€—è´¹çš„æ—¶é—´{:.0f}m{:.0f}s".format(time_use // 60,time_use%60))


    #é€‰æ‹©æœ€ä¼˜å‚æ•°
    #åŠ è½½æœ€é«˜å‡†ç¡®ç‡ä¸‹çš„æ¨¡å‹å‚æ•°
    LeNET.load_state_dict(best_model_wts)
    #ä¿å­˜å‚æ•°åŠæƒé‡
    torch.save(LeNET.load_state_dict(best_model_wts),'best_model.pth')#è¿™ä¸ªé‡Œé¢åŒ…å«æ¨¡å‹ç»“æ„å’Œå‚æ•°,ä¸€èˆ¬ä¸ºdataframe

    train_process = pd.DataFrame(data = {"epoch":range(num_epochs),
                                         "train_loss_all":train_loss_all,
                                         "val_loss_all":train_loss_all,
                                         "train_acc_all": train_loss_all,
                                         "val_acc_all": train_loss_all,
                                          })

    return train_process




def matplot_acc_loss(train_process):
    plt.figure(figsize = (12,4))
    plt.subplot(1,2,1)#å…¶å®å°±æ˜¯ç±»æ¯”matlabï¼Œè¿˜æŒºå®¹æ˜“ç†è§£çš„
    plt.plot(train_process["epoch"],train_process.train_loss_all,'ro-',label ="train loss")
    plt.plot(train_process["epoch"], train_process.val_loss_all, 'bs-', label="val loss")
    plt.legend()
    plt.xlabel("epoch")
    plt.ylabel("loss")

    
    plt.subplot(1, 2, 2)  # å…¶å®å°±æ˜¯ç±»æ¯”matlabï¼Œè¿˜æŒºå®¹æ˜“ç†è§£çš„
    plt.plot(train_process["epoch"], train_process.train_acc_all, 'ro-', label="train acc")
    plt.plot(train_process["epoch"], train_process.val_acc_all, 'bs-', label="val acc")
    plt.legend()
    plt.xlabel("epoch")
    plt.ylabel("acc")
    plt.legend()
    plt.show()


if __name__ == "__main__":
    #æ¨¡å‹å®ä¾‹åŒ–
    LeNet = LeNet()
    train_dataloader,val_dataloader = train_val_data_process()
    train_process=train_model_process(LeNet, train_dataloader, val_dataloader, 20)
    matplot_acc_loss(train_process)
