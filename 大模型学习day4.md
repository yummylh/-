# ğŸ“… 2025-12-09 å­¦ä¹ æ—¥å¿—

## ğŸ¯ ä»Šæ—¥ç›®æ ‡

- [x] å®Œæˆleetcodeä¸‰é“é¢˜ç›®
- [x] å¯¹transformæ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œå¹¶ä¸”ç”Ÿæˆäº†æ•°æ®æŸ¥çœ‹å™¨ã€æå–å™¨ã€ä»¥åŠè§£å¯†å™¨ã€‚
## ğŸ“ å­¦ä¹ å†…å®¹
1. å­—æ¯å¼‚ä½è¯åˆ†ç»„
è§£é¢˜æ€è·¯ï¼šæœ¬è´¨ä¸Šæ¥è¯´æ˜¯hashmapçš„åº”ç”¨ï¼Œå°†æ’åºåstrå­˜å…¥hashmapä¸­çš„keyã€‚ä¹Ÿå°±æ˜¯â€œaetâ€ï¼Œéå†strä¸­çš„å€¼ï¼Œå‡ºç°ç›¸åŒå­—æ¯å³åŠ å…¥åˆ°hashmapä¸­çš„keyâ€”valueä¹Ÿå°±æ˜¯â€œeatâ€â€œateâ€
æ€è€ƒæ­¥éª¤ä¸ºï¼š
![æ€è€ƒè¿‡ç¨‹å›¾](./image/25.12.9day4/leetcodeå­—æ¯å¼‚ä½åˆ†è¯å™¨_æ€è€ƒ.jpg)
å…¶ä¸­é¢˜ç›®è¦æ±‚ä¸ä»£ç æäº¤ä¸º
![é¢˜ç›®è¦æ±‚åŠä»£ç æˆªå›¾](./image/25.12.9day4/å­—æ¯å¼‚ä½è¯åˆ†ç»„-ä»£ç åŠ é¢˜ç›®.png)
2. æœ€é•¿è¿ç»­åºåˆ—
åŒæ ·è€ƒå¯Ÿhashmapï¼Œè¿™é‡Œhashmapå­˜æ”¾çš„keyä¾ç„¶æ˜¯å…ƒç´ çš„å¤§å°ï¼Œä½†æ˜¯key-valueå­˜æ”¾çš„æ˜¯ç›®å‰ä¸ºæ­¢è¿ç»­åºåˆ—çš„é•¿åº¦ã€‚éœ€è¦æ³¨æ„çš„æ˜¯æ’åºå®Œåéœ€è¦åŠæ—¶æ›´æ–°é•¿åº¦ï¼Œä¹Ÿå°±æ˜¯key_valueçš„å€¼ï¼Œå¹¶ä¸”æ›´æ–°å®Œå½“å‰çš„é•¿åº¦å€¼åï¼Œä¹Ÿè¦æŠŠä¸¤ç«¯çš„å€¼key-valueçš„å€¼æ›´æ–°ä¸ºå½“å‰é•¿åº¦å€¼
æ€è€ƒæ­¥éª¤ä¸ºï¼š
![æ€è€ƒè¿‡ç¨‹å›¾](./image/25.12.9day4/leecodeæœ€é•¿è¿ç»­åºåˆ—ï¼Œç§»åŠ¨0_æ€è€ƒ.jpg)
å…¶ä¸­é¢˜ç›®è¦æ±‚ä¸ä»£ç æäº¤ä¸º
![é¢˜ç›®è¦æ±‚åŠä»£ç æˆªå›¾](./image/25.12.9day4/æœ€é•¿è¿ç»­åºåˆ—-ä»£ç åŠ é¢˜ç›®.png)
3. ç§»åŠ¨0
è§£é¢˜æ€è·¯ï¼Œæœ¬è´¨ä¸Šæ¥è¯´æ˜¯åŒæŒ‡é’ˆçš„åº”ç”¨ï¼Œææ¸…æ¥šæŒ‡é’ˆæŒ‡å‘ä½ç½®ä»¥åŠå³ä½¿æ›´æ–°æŒ‡é’ˆå³å¯ï¼Œå°†é0æ•°å¾€å‰ç§»åŠ¨ï¼Œå°†0å¾€åç§»ï¼ˆæµ‹è¯•ç”¨ä¾‹ä¸­å·²ç»æ’å¥½åºï¼‰
æ€è€ƒæ­¥éª¤ä¸ºï¼š
![æ€è€ƒè¿‡ç¨‹å›¾](./image/25.12.9day4/leecodeæœ€é•¿è¿ç»­åºåˆ—ï¼Œç§»åŠ¨0_æ€è€ƒ.jpg)
å…¶ä¸­é¢˜ç›®è¦æ±‚ä¸ä»£ç æäº¤ä¸º
![é¢˜ç›®è¦æ±‚åŠä»£ç æˆªå›¾](./image/25.12.9day4/ç§»åŠ¨0-ä»£ç åŠ é¢˜ç›®.png)
4. å¯¹transformerçš„æ•°æ®è¿›è¡Œäº†é¢„å¤„ç†ï¼Œä»è·å–è¯­æ–™æ•°æ®ï¼Œæå–è¯­æ–™æ•°æ®å½¢æˆè®­ç»ƒæ–‡ä»¶å¤¹ï¼Œå…¶ä»£ç ä¸º
```python
import json

path = "../data/json/train.json"  # ä½ çš„è¿™æ®µæ–‡ä»¶
with open(path, "r", encoding="utf-8") as f:
    data = json.load(f)  # è‡ªåŠ¨æŠŠ \uXXXX è¿˜åŸ

# çœ‹å‰ä¸¤æ¡ï¼Œensure_ascii=False ç”¨äºâ€œæ‰“å°ä¸­æ–‡â€
print(json.dumps(data[:3], ensure_ascii=False, indent=2)) #æ‰“å°å‰ä¸‰è¡Œçš„æ•°æ®

# data çš„ç»“æ„åº”å½“æ˜¯ï¼š[[en, zh], [en, zh], ...]
print(type(data), len(data), type(data[0]), len(data[0]))


#è¯»å–jsonæ–‡ä»¶ä¸­çš„å¥å¼ï¼Œå…¶å®ç»™å®šçš„æ˜¯æ„å»ºè¯è¡¨çš„æ–¹æ³•ï¼ŒåŠæ˜¯[è‹±æ–‡ï¼Œä¸­æ–‡]
5. æ„å»ºæ•°æ®é›†
```python
import json  # å¯¼å…¥jsonæ¨¡å—ï¼Œç”¨äºå¤„ç†JSONæ ¼å¼çš„æ•°æ®

if __name__ == "__main__":  # ä¸»ç¨‹åºå…¥å£ï¼Œå½“è„šæœ¬è¢«ç›´æ¥æ‰§è¡Œæ—¶è¿è¡Œ
    files = ['train', 'dev', 'test']  # å®šä¹‰æ–‡ä»¶åˆ—è¡¨ï¼ŒåŒ…å«è®­ç»ƒé›†ã€å¼€å‘é›†å’Œæµ‹è¯•é›†
    ch_path = 'corpus.ch'  # ä¸­æ–‡è¯­æ–™åº“æ–‡ä»¶è·¯å¾„
    en_path = 'corpus.en'  # è‹±æ–‡è¯­æ–™åº“æ–‡ä»¶è·¯å¾„
    ch_lines = []  # ç”¨äºå­˜å‚¨ä¸­æ–‡å¥å­çš„åˆ—è¡¨
    en_lines = []  # ç”¨äºå­˜å‚¨è‹±æ–‡å¥å­çš„åˆ—è¡¨

    for file in files:  # éå†æ–‡ä»¶åˆ—è¡¨
        # åŠ è½½JSONæ ¼å¼çš„è¯­æ–™æ–‡ä»¶ï¼Œä½¿ç”¨utf-8ç¼–ç 
        corpus = json.load(open('./json/' + file + '.json', 'r', encoding="utf-8"))
        # éå†è¯­æ–™ä¸­çš„æ¯ä¸€é¡¹ï¼Œæå–è‹±æ–‡å’Œä¸­æ–‡å¥å­
        for item in corpus:
            en_lines.append(item[0] + '\n')  # å°†è‹±æ–‡å¥å­æ·»åŠ åˆ°åˆ—è¡¨ï¼Œå¹¶æ·»åŠ æ¢è¡Œç¬¦
            ch_lines.append(item[1] + '\n')  # å°†ä¸­æ–‡å¥å­æ·»åŠ åˆ°åˆ—è¡¨ï¼Œå¹¶æ·»åŠ æ¢è¡Œç¬¦

    # å°†ä¸­æ–‡å¥å­å†™å…¥æ–‡ä»¶
    with open(ch_path, "w", encoding="utf-8") as fch:
        fch.writelines(ch_lines)  # ä¸€æ¬¡æ€§å†™å…¥æ‰€æœ‰ä¸­æ–‡å¥å­

    # å°†è‹±æ–‡å¥å­å†™å…¥æ–‡ä»¶
    with open(en_path, "w", encoding="utf-8") as fen:
        fen.writelines(en_lines)  # ä¸€æ¬¡æ€§å†™å…¥æ‰€æœ‰è‹±æ–‡å¥å­

    # è¾“å‡ºä¸­æ–‡å¥å­çš„è¡Œæ•°
    # lines of Chinese: 252777
    print("lines of Chinese: ", len(ch_lines))
    # è¾“å‡ºè‹±æ–‡å¥å­çš„è¡Œæ•°
    # lines of English: 252777
    print("lines of English: ", len(en_lines))
    # è¾“å‡ºå®Œæˆæç¤ºä¿¡æ¯
    print("-------- Get Corpus ! --------")
6.å¯¹æ•°æ®é›†è¿›è¡Œåˆ†æä»¥åŠéªŒè¯æ•°æ®é›†æ˜¯å¦å­˜åœ¨
```python
import os


def analyze_corpus(ch_path, en_path):
    """
    åˆ†æåŒè¯­è¯­æ–™åº“æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
    Args:
        ch_path: ä¸­æ–‡æ–‡ä»¶è·¯å¾„
        en_path: è‹±æ–‡æ–‡ä»¶è·¯å¾„
    """
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(ch_path):
        print(f"ä¸­æ–‡æ–‡ä»¶ä¸å­˜åœ¨: {ch_path}")
        return
    if not os.path.exists(en_path):
        print(f"è‹±æ–‡æ–‡ä»¶ä¸å­˜åœ¨: {en_path}")
        return

    # è·å–æ–‡ä»¶å¤§å°
    ch_size = os.path.getsize(ch_path)
    en_size = os.path.getsize(en_path)

    # è¯»å–æ–‡ä»¶å†…å®¹å¹¶ç»Ÿè®¡è¡Œæ•°
    with open(ch_path, 'r', encoding='utf-8') as f:
        ch_lines = f.readlines()
    with open(en_path, 'r', encoding='utf-8') as f:
        en_lines = f.readlines()

    # è®¡ç®—å­—ç¬¦æ•°ï¼ˆä¸åŒ…æ‹¬æ¢è¡Œç¬¦ï¼‰
    ch_chars = sum(len(line.strip()) for line in ch_lines)
    en_chars = sum(len(line.strip()) for line in en_lines)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("=" * 50)
    print("è¯­æ–™åº“ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 50)
    print(f"\nä¸­æ–‡æ–‡ä»¶ ({ch_path}):")
    print(f"  æ–‡ä»¶å¤§å°: {ch_size / 1024 / 1024:.2f} MB")
    print(f"  æ€»è¡Œæ•°: {len(ch_lines):,}")
    print(f"  æ€»å­—ç¬¦æ•°: {ch_chars:,}")
    print(f"  å¹³å‡æ¯è¡Œå­—ç¬¦æ•°: {ch_chars / len(ch_lines):.1f}")

    print(f"\nè‹±æ–‡æ–‡ä»¶ ({en_path}):")
    print(f"  æ–‡ä»¶å¤§å°: {en_size / 1024 / 1024:.2f} MB")
    print(f"  æ€»è¡Œæ•°: {len(en_lines):,}")
    print(f"  æ€»å­—ç¬¦æ•°: {en_chars:,}")
    print(f"  å¹³å‡æ¯è¡Œå­—ç¬¦æ•°: {en_chars / len(en_lines):.1f}")

    # éªŒè¯ä¸­è‹±æ–‡è¡Œæ•°æ˜¯å¦åŒ¹é…
    if len(ch_lines) == len(en_lines):
        print("\nâœ“ ä¸­è‹±æ–‡æ–‡ä»¶è¡Œæ•°åŒ¹é…")
    else:
        print("\nâœ— è­¦å‘Šï¼šä¸­è‹±æ–‡æ–‡ä»¶è¡Œæ•°ä¸åŒ¹é…ï¼")

    print("=" * 50)


if __name__ == "__main__":
    ch_path = 'corpus.ch'
    en_path = 'corpus.en'
    analyze_corpus(ch_path, en_path)
>æ˜å¤©ç»§ç»­leetcodeåˆ·ä¸‰é“é¢˜
>ç»§ç»­å®ç°transformerçš„tokenizeråˆ†è¯å™¨ç»“æ„
