1、今天学习了transformer的基本框架结构以及对应的用法
  encoder部分
  一）、词嵌入与位置编码
  本质上来说是将文字数字化的过程，产生对应的数字矩阵，位置编码给出了句子中每个单词之间的“距离”
  二）、自注意力机制（self_Attention）
  主要作用是计算各个文字矩阵中文字间的关联度，其实就是三个参数矩阵，分别是Q矩阵、K矩阵、V矩阵。
  重要的点在于在进行计算自注意力得分矩阵时，需要÷（dk）^1/2，是因为防止softmax函数饱和，导致梯度消失，随之引起的参数更新缓慢
  由此引申出多头注意力机制(Muti_head_Attention）
  其实就是多个参数矩阵，多个Q矩阵、多个K矩阵、多个V矩阵，多头的作用是在不同子空间、不同相对位置下做不同对齐；通俗一点就是在不同环境下对长依赖、短依赖、句子标点、句子句法等进行不同程度的关注
               注意力机制的输入可以是不同长度的序列，符合机器翻译等时间序列任务
  三）、填充掩码
  将输入在同一批次的句子长度填充为一致长度，方便GPU进行并行计算。
  四）、层归一化（Layer-Normalization）
  将一个批次中每一个样本经过的神经网络进行整体归一化处理，使用原因是因为模型处理的是变长的序列，批归一化在处理此类序列时会受到限制。
  这里会引起和批归一化（Batch_Normalization）歧义；批归一化指的是一个批次中，每一个样本数据同一个神经元进行归一化处理。
  五）、前馈神经网络
  一种结构类型，而不是特定的网络，其实就是前向传播，从输入到输出的单向流动。
  decoder部分
  六）、掩码注意力机制
  在模型中，decoder输入的是label，为了防止数据泄露，使用0将有关信息进行填充掩盖，通俗来说，就是第一位的数据只能看见自己，第二位的数据能看见第一位数据和第二位本身，不具备看到后续数据的能力。
  七）、交叉注意力机制
  通俗来说就是在encoder输出自注意力矩阵A时，最后的信息融合矩阵乘的V矩阵不来自encoder，而来自decoder的V矩阵，二者相乘得到信息融合矩阵
2、leetcode部分
一）、梦开始的地方，两数之和，题目为：给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。

你可以假设每种输入只会对应一个答案，并且你不能使用两次相同的元素。

你可以按任意顺序返回答案。
               示例 1：

输入：nums = [2,7,11,15], target = 9
输出：[0,1]
解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。
解题过程：因为考虑到遍历数组返回元素索引index，并且需要判断这个数是否已经遍历过，是否在这个集合出现过、所以考虑hash_map
               需要注意的是，hash表中的key以及key_value与所给的index中的下标和元素值不同，因为题目要求返回的是元素下标内容，所以此处我们将hash表中的key_value设为index中的下标，hash表中的key设为index中的元素数值，可以得到如下代码。
class Solution:
    def twoSum(self, nums: List[int], target: int) -> List[int]:#例如nums = [2,7,3,6]
        record = dict()#创建hash表，需要注意的是，因为这里需要返回的是原数组索引的index，所以hashmap中的key_value应该是原数组index，key是原数组的value
        for index ,value in enumerate(nums):#enumerate返回的是nums数组中的索引+数值，也就是例如(0,2)(1,7)(2,3)(3,6)
            if target - value in record:
                return[record[target - value],index]#返回哈希表中的键值（原数组所以）以及当前的索引
            else:
                record[value] = index#不然就将现在指针所指的索引存入hash表中的key_value
        return[]               
               
               
